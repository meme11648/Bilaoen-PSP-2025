---
title: "Climate Change Data"
author: "Nancy Carmona"
date: "`r format(Sys.time(), '%B %d, %Y')`"
output:
  pdf_document: default
  html_document: default
---


```{r set.knitr.options, echo=FALSE, message=FALSE, warning=FALSE}
#-----set knitr options-----

# clear environment
rm(list = ls())

# knitr options
suppressMessages(library(knitr))
opts_chunk$set(tidy = FALSE, echo = TRUE, message = FALSE)

# Load pacman, installing if needed and other packages as needed
if (!require("pacman")) { install.packages("pacman", repos = "http://cran.r-project.org") }

# Load other packages, installing as needed.
pacman::p_load(tidyverse, dplyr, httr, jsonlite, tigris, sf, gstat)

# # set working directory
# work_dir <- getwd()
# 
# # name and create output directory
# output_dir <- file.path(work_dir, "output")
# dir.create(output_dir, showWarnings = TRUE, recursive = TRUE)

```


# Introduction

This code will walk through obtaining weather data from NOAA and Daymet. 

## First, you will need to obtain an API Token to use the NOAA website. 

```{r token, echo = FALSE}
# Set your NOAA API Token (replace with your own API key)
noaa_token <- "sYSaRlnFshYqsTcobQCAivNIaHqEUWVc"

# Define the API endpoint URL (get this from NOAA website https://www.ncdc.noaa.gov/cdo-web/webservices/v2)

# base_url <- "https://www.ncdc.noaa.gov/cdo-web/api/v2/data"

# url from NOAA website, either can be used https://www.ncdc.noaa.gov/cdo-web/webservices/v2


```


## Next you can download data from the API. 

The following code includes a function to download the data. 

```{r define API, echo = FALSE}

noaa_token <- "sYSaRlnFshYqsTcobQCAivNIaHqEUWVc"


fetch_noaa_data_paginated <- function(start_date, end_date, location_id, datatypeids, token) {
  base_url <- "https://www.ncdc.noaa.gov/cdo-web/api/v2/data"
  all_results <- list()
  offset <- 1
  limit <- 1000
  more_data <- TRUE
  
  # Create the base query
  params <- list(
    datasetid = "GHCND",
    locationid = location_id,
    startdate = start_date,
    enddate = end_date,
    units = "metric",
    limit = limit,
    offset = offset
  )
  
  # Build the query string with multiple datatypeids
  datatypeid_params <- paste(paste0("datatypeid=", datatypeids), collapse = "&")
  
  while(more_data) {
    # Build the full URL with parameters
    full_url <- modify_url(
      base_url,
      query = c(params, list(offset = offset))
    )
    # Add the datatypeids manually
    full_url <- paste0(full_url, "&", datatypeid_params)
    
    response <- GET(full_url, add_headers(token = token))
    if (status_code(response) != 200) {
      cat("Error: Status", status_code(response), "\n")
      break
    }
    results <- fromJSON(content(response, "text", encoding = "UTF-8"))
    if (!is.null(results$results)) {
      all_results <- append(all_results, list(results$results))
      num_results <- length(results$results)
      cat("Fetched", num_results, "records (offset", offset, ")\n")
      offset <- offset + limit
      if (num_results < limit) {
        more_data <- FALSE
      }
    } else {
      more_data <- FALSE
    }
    Sys.sleep(1)
  }
  if (length(all_results) > 0) {
    return(bind_rows(all_results))
  } else {
    return(NULL)
  }
}



```


## The function can be used based on the year, location id, and data type. These inputs can be changed depending on what you would like to download. The code below is looking at the city code for San Francisco.  

```{r api.using.function, echo = FALSE}
# Example usage (loop by year, API works better if 1 year or less of data being requested at a time)
all_data <- list()
for (yr in 2000:2024) {
  cat("Downloading data for year:", yr, "\n")
  res <- fetch_noaa_data_paginated(
    start_date = paste0(yr, "-01-01"),
    end_date = paste0(yr, "-12-31"),
    location_id = "CITY:US060031",
    datatypeids = c("TMAX", "TMIN"),
    token = noaa_token
  )
  if (!is.null(res)) {
    all_data[[as.character(yr)]] <- res
  }
}
sf_weather <- bind_rows(all_data)

# save the data as a CSV to use for later 
write.csv(sf_weather, "sf_weather_noaa_2000_2024.csv", row.names = FALSE)
```


The city had many stations outside of San Francisco. The next thing we tried was using the FIPS code for San Francisco county. 

```{r use.fips.id, echo = FALSE}

fetch_noaa_data_paginated <- function(start_date, end_date, location_id, datatypeids, token) {
  base_url <- "https://www.ncdc.noaa.gov/cdo-web/api/v2/data"
  all_results <- list()
  offset <- 1
  limit <- 1000
  more_data <- TRUE
  
  # Create the base query
  params <- list(
    datasetid = "GHCND",
    locationid = location_id,
    startdate = start_date,
    enddate = end_date,
    units = "metric",
    limit = limit,
    offset = offset
  )
  
  # Build the query string with multiple datatypeids
  datatypeid_params <- paste(paste0("datatypeid=", datatypeids), collapse = "&")
  
  while(more_data) {
    # Build the full URL with parameters
    full_url <- modify_url(
      base_url,
      query = c(params, list(offset = offset))
    )
    # Add the datatypeids manually
    full_url <- paste0(full_url, "&", datatypeid_params)
    
    response <- GET(full_url, add_headers(token = token))
    if (status_code(response) != 200) {
      cat("Error: Status", status_code(response), "\n")
      break
    }
    results <- fromJSON(content(response, "text", encoding = "UTF-8"))
    if (!is.null(results$results)) {
      all_results <- append(all_results, list(results$results))
      num_results <- length(results$results)
      cat("Fetched", num_results, "records (offset", offset, ")\n")
      offset <- offset + limit
      if (num_results < limit) {
        more_data <- FALSE
      }
    } else {
      more_data <- FALSE
    }
    Sys.sleep(1)
  }
  if (length(all_results) > 0) {
    return(bind_rows(all_results))
  } else {
    return(NULL)
  }
}

# Example usage (loop by year, API works better if 1 year or less of data being requested at a time)
all_data <- list()
for (yr in 2000:2024) {
  cat("Downloading data for year:", yr, "\n")
  res <- fetch_noaa_data_paginated(
    start_date = paste0(yr, "-01-01"),
    end_date = paste0(yr, "-12-31"),
    location_id = "FIPS:06075",
    datatypeids = c("TMAX", "TMIN"),
    token = noaa_token
  )
  if (!is.null(res)) {
    all_data[[as.character(yr)]] <- res
  }
}
sf_weather <- bind_rows(all_data)


# save the data as a CSV to use for later 
write.csv(sf_weather, "sf_weather_noaa_2000_2024.csv", row.names = FALSE)

```


### Note: we figured out what station ids were in San Francisco 
```{r use.station.id, echo = FALSE}


# c("GHCND:US1CASF0003", "GHCND:US1CASF0004", "GHCND:US1CASF0008", "GHCND:US1CASF0014", "GHCND:US1CASF0017", "GHCND:US1CASF0020", "GHCND:US1CASF0021", "GHCND:USC00047765", "GHCND:USC00047767", "GHCND:USW00023272")


```


## Join weather data with station data. 

```{r combine.data, echo = FALSE}

response <- GET(
  url = "https://www.ncei.noaa.gov/cdo-web/api/v2/stations",
  query = list(
    locationid = "FIPS:06075",   # San Francisco city code
    datasetid = "GHCND",
    limit = 1000
  ),
  add_headers(token = noaa_token)
)

stations_json <- fromJSON(content(response, "text", encoding = "UTF-8"))
sf_stations_api <- stations_json$results

# Show the resulting stations
sf_station_ids <- sf_stations_api$id
sf_weather_only <- sf_weather %>% filter(station %in% sf_station_ids)
# Convert sf_stations_api to a tibble/data frame if not already
sf_stations_df <- as_tibble(sf_stations_api)

# Join weather data to station metadata (by station ID)
sf_weather_with_meta <- sf_weather_only %>%
  left_join(sf_stations_df, by = c("station" = "id"))


# Convert to Spatial Data
weather_points <- st_as_sf(sf_weather_with_meta, coords = c("longitude", "latitude"), crs = 4326)


# Get 2020 census tracts for San Francisco County (FIPS: 075)
sf_tracts <- tracts(state = "CA", county = "San Francisco", year = 2020, class = "sf")

plot(sf_tracts$geometry)

sf_tracts <- st_transform(sf_tracts, crs = 4326)


```

## We then interpolated the station data to get data for each census tract within San Francisco. 

```{r interpolate, echo = FALSE}
library(sf)
library(gstat)
library(dplyr)
library(purrr)

tract_centroids <- st_centroid(sf_tracts)

interpolate_one <- function(datatype, date) {
  day_pts <- weather_points %>%
    filter(datatype == !!datatype, date == !!date, !is.na(value))
  if (nrow(day_pts) < 2) return(NULL) # skip if not enough points
  
  # Make sure CRS matches
  day_pts <- st_transform(day_pts, 4326)
  centroids <- st_transform(tract_centroids, 4326)
  
  idw_result <- gstat::idw(
    value ~ 1,
    day_pts,
    newdata = centroids,
    idp = 2.0
  )
  tibble(
    GEOID = centroids$GEOID,
    date = date,
    datatype = datatype,
    interpolated_value = idw_result$var1.pred
  )
}

# Loop as before using purrr
datatypes <- unique(weather_points$datatype)
all_dates <- unique(weather_points$date)
grid <- expand.grid(datatype = datatypes, date = all_dates, stringsAsFactors = FALSE)
interpolated_results <- pmap_dfr(
  grid,
  function(datatype, date) interpolate_one(datatype, date)
)

```


```{r combine.tracts, echo= FALSE}

sf_tracts_with_weather <- left_join(sf_tracts, interpolated_results, by = "GEOID")

```


## We can also transform units from metric to Fahrenheit

```{r, transform.f, echo = FALSE}
library(dplyr)

sf_tracts_with_weather <- sf_tracts_with_weather %>%
  mutate(
    interpolated_value_f = (interpolated_value / 10) * 9/5 + 32, # check the calculation 
  )

```

## Fixing Time to have a more useful timestamp. 

```{r, transform.time, echo = FALSE}
library(dplyr)
library(lubridate)

sf_tracts_with_weather_time <- sf_tracts_with_weather %>%
  mutate(
    # parse the ISO string as UTC
    datetime_utc = ymd_hms(date, tz = "UTC"),
    # shift that instant into Pacific Time (handles PST vs PDT)
    datetime_pst = with_tz(datetime_utc, tzone = "America/Los_Angeles"),
    # now pull out each component in local time
    date_pst = as.Date(datetime_pst),
    Year   = year(datetime_pst),
  ) %>%
  # optional: drop the helper columns if you donâ€™t need them
  select(-datetime_utc)

```


## We can also plot the weater data using ggplot2. 

```{r, plot.weather, echo = FALSE}
library(ggplot2)
library(dplyr)

target_date <- "1990-08-01" # selecting a specific date 

# If you want to map all datatypes (e.g., TMAX & TMIN), you can facet by datatype
tracts_day <- sf_tracts_with_weather_time %>%
  filter(date_pst == target_date)

ggplot(tracts_day) +
  geom_sf(aes(fill = interpolated_value_f), color = "white", size = 0.1) +
  scale_fill_viridis_c(
    na.value = "grey80",
    name = "Weather value"
  ) +
  labs(
    title = paste("Interpolated Weather by Tract on", target_date),
    fill = "Value"
  ) +
  facet_wrap(~datatype) +         # Remove this line if you only have one datatype!
  theme_minimal()

```


## We can also download the package `rnoaa` instead of using the API. 
 
```{r}

install.packages("~/Library/Mobile Documents/com~apple~CloudDocs/Documents/SFSU Faculty/PINC 2025/PSP2025/rnoaa_1.4.0.tar.gz", type = "source", repos = NULL)

pacman::p_load(crul, XML, isdparser, geonames, hoardr)

library(rnoaa)

```

```{r}
options(noaakey = "sYSaRlnFshYqsTcobQCAivNIaHqEUWVc")

ncdc_locs(locationcategoryid='CITY', sortfield='name', sortorder='desc')


ncdc_stations(datasetid='GHCND', locationid='FIPS:06075')

sf_stations <- ncdc_stations(datasetid='GHCND', locationid='FIPS:06075')

sf_stations_df <- sf_stations$data$id

# SF county FIPS 06075 

# City ID CITY:US060031


ncdc_stations(datasetid='GHCND', locationid='FIPS:06075') 


lapply(c("GHCND:US1CASF0003", "GHCND:US1CASF0004", "GHCND:US1CASF0008", "GHCND:US1CASF0014", "GHCND:US1CASF0017", "GHCND:US1CASF0020", "GHCND:US1CASF0021", "GHCND:USC00047765", "GHCND:USC00047767", "GHCND:USW00023272"), function(z) {
  ncdc_stations(
   startdate = "2013-01-01",
   enddate = "2014-11-01")
}$data)


# to do: plot the 10 locations and verify that they are in SF 

# 1 %in% unlist(A)

sf_stations_df %in% df_data_stations

df_data_stations %in% sf_stations_df


out <- ncdc(
  datasetid = 'NORMAL_DLY',
  stationid = 'GHCND:USW00023272', # downtown SF 
  datatypeid = 'dly-tmax-normal',
  startdate = '2000-01-01',
  enddate = '2000-02-01'
)


ncdc_datacats(locationid = 'CITY:US060031')


sf_data_2000 <- ncdc(datasetid = "NORMAL_DLY", stationid = sf_stations$data$id[1:10],
        datatypeid='dly-tmax-normal',
   startdate = "2000-01-01", enddate = "2000-12-31")


```



## Daymet R package

```{r}

if(!require(remotes)){install.packages("remotes")}
remotes::install_github("bluegreen-labs/daymetr")
library("daymetr")

# Bounding box:  xmin: -123.1738 ymin: 37.63983 xmax: -122.2818 ymax: 37.92982


download_daymet_ncss(location = c(37.63983, -123.1738, 37.92982, -122.2818),
                     start = 2000,
                     end = 2001,
                     param = "tmax")


download_daymet_tiles(location = c(36.0133,-84.2625),
                      tiles = NULL,
                      start = 1980,
                      end = 2012,
                      param = "ALL")

```



```{r session.info, echo=FALSE}
#-----session info-----

sessionInfo()
```


