---
title: "Climate Change Data"
author: "Nancy Carmona"
date: "`r format(Sys.time(), '%B %d, %Y')`"
output:
  pdf_document: default
  html_document: default
---


```{r set.knitr.options, echo=FALSE, message=FALSE, warning=FALSE}
#-----set knitr options-----

# clear environment
rm(list = ls())

# knitr options
suppressMessages(library(knitr))
opts_chunk$set(tidy = FALSE, echo = TRUE, message = FALSE)

# Load pacman, installing if needed and other packages as needed
if (!require("pacman")) { install.packages("pacman", repos = "http://cran.r-project.org") }

# Load other packages, installing as needed.
pacman::p_load(tidyverse, dplyr, httr, jsonlite, tigris, sf, gstat)

# # set working directory
# work_dir <- getwd()
# 
# # name and create output directory
# output_dir <- file.path(work_dir, "output")
# dir.create(output_dir, showWarnings = TRUE, recursive = TRUE)

```


# Introduction

This code will walk through obtaining weather data from NOAA and Daymet. 

## First, you will need to obtain an API Token to use the NOAA website. 

```{r token, echo = FALSE}
# Set your NOAA API Token (replace with your own API key)
noaa_token <- "sYSaRlnFshYqsTcobQCAivNIaHqEUWVc"

# Define the API endpoint URL (get this from NOAA website https://www.ncdc.noaa.gov/cdo-web/webservices/v2)

# base_url <- "https://www.ncdc.noaa.gov/cdo-web/api/v2/data"

# url from NOAA website, either can be used https://www.ncdc.noaa.gov/cdo-web/webservices/v2


```


## Next you can download data from the API. 

The following code includes a function to download the data. 

```{r api.function, eval = FALSE}

noaa_token <- "sYSaRlnFshYqsTcobQCAivNIaHqEUWVc"


fetch_noaa_data_paginated <- function(start_date, end_date, location_id, datatypeids, token) {
  base_url <- "https://www.ncdc.noaa.gov/cdo-web/api/v2/data"
  all_results <- list()
  offset <- 1
  limit <- 1000
  more_data <- TRUE
  
  # Create the base query
  params <- list(
    datasetid = "GHCND",
    locationid = location_id,
    startdate = start_date,
    enddate = end_date,
    units = "metric",
    limit = limit,
    offset = offset
  )
  
  # Build the query string with multiple datatypeids
  datatypeid_params <- paste(paste0("datatypeid=", datatypeids), collapse = "&")
  
  while(more_data) {
    # Build the full URL with parameters
    full_url <- modify_url(
      base_url,
      query = c(params, list(offset = offset))
    )
    # Add the datatypeids manually
    full_url <- paste0(full_url, "&", datatypeid_params)
    
    response <- GET(full_url, add_headers(token = token))
    if (status_code(response) != 200) {
      cat("Error: Status", status_code(response), "\n")
      break
    }
    results <- fromJSON(content(response, "text", encoding = "UTF-8"))
    if (!is.null(results$results)) {
      all_results <- append(all_results, list(results$results))
      num_results <- length(results$results)
      cat("Fetched", num_results, "records (offset", offset, ")\n")
      offset <- offset + limit
      if (num_results < limit) {
        more_data <- FALSE
      }
    } else {
      more_data <- FALSE
    }
    Sys.sleep(1)
  }
  if (length(all_results) > 0) {
    return(bind_rows(all_results))
  } else {
    return(NULL)
  }
}



```


## The function can be used based on the year, location id, and data type. These inputs can be changed depending on what you would like to download. The code below is looking at the city code for San Francisco.  

```{r use.city.id, eval = FALSE}
# Example usage (loop by year, API works better if 1 year or less of data being requested at a time)
# all_data <- list()
# for (yr in 2000:2024) {
#   cat("Downloading data for year:", yr, "\n")
#   res <- fetch_noaa_data_paginated(
#     start_date = paste0(yr, "-01-01"),
#     end_date = paste0(yr, "-12-31"),
#     location_id = "CITY:US060031", # the city ID provided many locations outside of the city 
#     datatypeids = c("TMAX", "TMIN"),
#     token = noaa_token
#   )
#   if (!is.null(res)) {
#     all_data[[as.character(yr)]] <- res
#   }
# }
# sf_weather <- bind_rows(all_data)

# save the data as a CSV to use for later 
# write.csv(sf_weather, "sf_weather_noaa_2000_2024.csv", row.names = FALSE)
```


The city had many stations outside of San Francisco. The next thing we tried was using the FIPS code for San Francisco county. 

```{r use.fips.id, echo = FALSE}

# Example usage (loop by year, API works better if 1 year or less of data being requested at a time)
all_data <- list()
for (yr in 2000:2024) {
  cat("Downloading data for year:", yr, "\n")
  res <- fetch_noaa_data_paginated(
    start_date = paste0(yr, "-01-01"),
    end_date = paste0(yr, "-12-31"),
    location_id = "FIPS:06075", # FIPS for SF
    datatypeids = c("TMAX", "TMIN", "TAVG"),
    token = noaa_token
  )
  if (!is.null(res)) {
    all_data[[as.character(yr)]] <- res
  }
}
sf_weather_new <- bind_rows(all_data)


# save the data as a CSV to use for later 
#write.csv(sf_weather, "sf_weather_noaa_2000_2024.csv", row.names = FALSE)

```


## Exploring SF Weather Data 

Weekly heat wave intensity. We will calculate the intensity and duration of heat waves on a weekly basis for each census tract using daily weather data. Heat waves will be characterized using established methods and our own criteria. 

We will use the earliest available temperature data for each census tract to determine shifts in heat wave frequency and intensity due to climate change. 

We will also examine other temperature statistics, such as mean daily temperature, nighttime lows, and daily temperature range. 


**Criteria for Heatwave** 

A heat wave is defined as 2 or more consecutive days where the daily maximum temperature exceeds the 95th percentile of the long-term temperature distribution for that location and time of year.

```{r}

# read in data that was previously downloaded (the date variable will be different format)
# sf_weather <- read_csv("Climate Change/sf_weather_noaa_2000_2024.csv")

# look over all data 
summary(sf_weather)

# look at number of stations 
table(sf_weather$station)

# check min and max dates 
min(sf_weather$date) # "2000-01-01T00:00:00"

max(sf_weather$date) # "2023-11-20T00:00:00"

# can also check the range of dates 
range(sf_weather$date)

# look at first 6 rows of data 
head(sf_weather)

# transform Celsius to Fahrenheit
## Temperature in degrees Fahrenheit (°F) = (Temperature in degrees Celsius (°C) * 9/5) + 32

sf_weather <- sf_weather %>% 
                mutate(temp_f = (value * (9/5) + 32))

sf_weather <- sf_weather %>%  rename(temp_c = value)

# dealing with time from direct download 
# sf_weather <- sf_weather %>%
#   mutate(date = as_date(with_tz(ymd_hms(date, tz = "UTC"), tzone = "America/Los_Angeles")))

# dealing with time from csv file 
sf_weather <- sf_weather %>% 
                mutate(date = ymd(date)) 

# plot using ggplot2 

ggplot(sf_weather, aes(x = date, y = temp_f, color = datatype)) + geom_point()

# wrapping by data type (min or max)
ggplot(sf_weather, aes(x = date, y = temp_f, color = datatype)) + geom_point() + 
  facet_wrap(~datatype)

# wrapping by station 
ggplot(sf_weather, aes(x = date, y = temp_f, color = datatype)) + geom_point() + 
  facet_wrap(~station)

# add axis labels, customize axis ticks 

ggplot(sf_weather, aes(x = date, y = temp_f, color = datatype)) +
  geom_point() +
  scale_x_date(
    breaks = seq(from = as.Date("2000-01-01"), to = as.Date("2025-01-01"), by = "5 years"),
    date_labels = "%Y"  # Only show the year
  ) +
labs(y = "Temperature (\u00B0F)") + 
  xlab("Date") + 
  ggtitle("Temperature in SF 2000-2024") + 
    theme(plot.title = element_text(hjust = 0.5)) + 
  labs(color = "Type")


```

We can aggregate the daily data to weekly data to help visualize it. 

```{r}

# average weekly min and max temperature 
df_weekly <- sf_weather %>%
  # Convert 'date' to Date format if not already
  mutate(date = as.Date(date)) %>%
  # Floor the date to the start of the week (default = Sunday, use 'week_start = 1' for Monday)
  mutate(week_start = floor_date(date, unit = "week", week_start = 1)) %>%
  # Group by week, station, and datatype
  group_by(station, datatype, week_start) %>%
  # Summarize weekly temperature
  summarise(
    weekly_temp_c = mean(temp_c, na.rm = TRUE),
    weekly_temp_f = mean(temp_f, na.rm = TRUE),
    .groups = "drop"
  )



ggplot(df_weekly, aes(x = week_start, y = weekly_temp_f, color = datatype)) +
  geom_point() +
  scale_x_date(
    breaks = seq(from = as.Date("2000-01-01"), to = as.Date("2025-01-01"), by = "5 years"),
    date_labels = "%Y"  # Only show the year
  ) +
labs(y = "Temperature (\u00B0F)") + 
  xlab("Date") + 
  ggtitle("Weekly Temp in SF 2000-2024") + 
    theme(plot.title = element_text(hjust = 0.5)) + 
  labs(color = "Type")


```

We can aggregate the daily data to bi-weekly data to help visualize it. 

```{r}

# Assume your dataframe is named `df`
df_biweekly <- sf_weather %>%
  # Ensure date is in Date format
  mutate(date = as.Date(date)) %>%
  # Get week start date
  mutate(week_start = floor_date(date, unit = "week", week_start = 1)) %>%
  # Compute 2-week group: number of weeks since origin, then floor by 2-week bins
  mutate(week_num = as.numeric(difftime(week_start, min(week_start), units = "weeks")),
         biweek_start = min(week_start) + weeks(floor(week_num / 2) * 2)) %>%
  # Group by biweekly start date, station, and datatype
  group_by(station, datatype, biweek_start) %>%
  # Summarize mean temperature for 2-week periods
  summarise(
    temp_c_biweek_avg = mean(temp_c, na.rm = TRUE),
    temp_f_biweek_avg = mean(temp_f, na.rm = TRUE),
    .groups = "drop"
  )


ggplot(df_biweekly, aes(x = biweek_start, y = temp_f_biweek_avg, color = datatype)) +
  geom_point() +
  scale_x_date(
    breaks = seq(from = as.Date("2000-01-01"), to = as.Date("2025-01-01"), by = "5 years"),
    date_labels = "%Y"  # Only show the year
  ) +
labs(y = "Temperature (\u00B0F)") + 
  xlab("Date") + 
  ggtitle("Biweekly Temp in SF 2000-2024") + 
    theme(plot.title = element_text(hjust = 0.5)) + 
  labs(color = "Type")


```





### Note: we figured out what station ids were in San Francisco 
```{r use.station.id, echo = FALSE}


# c("GHCND:US1CASF0003", "GHCND:US1CASF0004", "GHCND:US1CASF0008", "GHCND:US1CASF0014", "GHCND:US1CASF0017", "GHCND:US1CASF0020", "GHCND:US1CASF0021", "GHCND:USC00047765", "GHCND:USC00047767", "GHCND:USW00023272")


```


## Combine weather and station data

We then join weather data with station data, this will add the lat/long data needed for next step in the interpolation. 

```{r combine.data, echo = FALSE}

# get list of stations in san francisco 

response <- GET(
  url = "https://www.ncei.noaa.gov/cdo-web/api/v2/stations",
  query = list(
    locationid = "FIPS:06075",   # San Francisco city code
    datasetid = "GHCND",
    limit = 1000
  ),
  add_headers(token = noaa_token)
)

stations_json <- fromJSON(content(response, "text", encoding = "UTF-8"))
sf_stations_api <- stations_json$results

# Show the resulting stations
sf_station_ids <- sf_stations_api$id
sf_weather_only <- sf_weather %>% filter(station %in% sf_station_ids)
# Convert sf_stations_api to a tibble/data frame if not already
sf_stations_df <- as_tibble(sf_stations_api)

# Join weather data to station metadata (by station ID)
sf_weather_with_meta <- sf_weather_only %>%
  left_join(sf_stations_df, by = c("station" = "id"))


# Convert to Spatial Data
weather_points <- st_as_sf(sf_weather_with_meta, coords = c("longitude", "latitude"), crs = 4326)


# Get 2020 census tracts for San Francisco County (FIPS: 075)
sf_tracts <- tracts(state = "CA", county = "San Francisco", year = 2020, class = "sf")

# plot(sf_tracts$geometry)

sf_tracts <- st_transform(sf_tracts, crs = 4326)


```

## Interpolation of points to census tracts

We then interpolated the station data to get data for each census tract within San Francisco. 

```{r interpolate, echo = FALSE}

# read in downloaded csv of data 
# sf_weather <- read_csv("Climate Change/sf_weather_noaa_2000_2024.csv")


library(sf)
library(gstat)
library(dplyr)
library(purrr)

tract_centroids <- st_centroid(sf_tracts)

interpolate_one <- function(datatype, date, value) {
  day_pts <- weather_points %>%
    filter(datatype == !!datatype, date == !!date, !is.na(value))
  if (nrow(day_pts) < 2) return(NULL) # skip if not enough points
  
  # Make sure CRS matches
  day_pts <- st_transform(day_pts, 4326)
  centroids <- st_transform(tract_centroids, 4326)
  
  idw_result <- gstat::idw(
    value ~ 1,
    day_pts,
    newdata = centroids,
    idp = 2.0
  )
  tibble(
    GEOID = centroids$GEOID,
    date = date,
    datatype = datatype,
    interpolated_value = idw_result$var1.pred
  )
}

# Loop as before using purrr
datatypes <- unique(weather_points$datatype)
all_dates <- unique(weather_points$date)

grid <- expand.grid(datatype = datatypes, date = all_dates, stringsAsFactors = FALSE)

interpolated_results <- pmap_dfr(
  grid,
  function(datatype, date) interpolate_one(datatype, date, value)
)

```


# Combine weather with 
```{r combine.tracts, echo= FALSE}

sf_tracts_with_weather <- left_join(sf_tracts, interpolated_results, by = "GEOID")

```


## Transform metric units 

We can also transform units from metric to Fahrenheit

```{r, transform.f, echo = FALSE}
library(dplyr)

sf_tracts_with_weather <- sf_tracts_with_weather %>%
  mutate(
    interpolated_value_f = (interpolated_value) * 9/5 + 32, # check the calculation 
  )

```

## Transform time format 

Fixing Time to have a more useful timestamp. 

```{r, transform.time, echo = FALSE}
library(dplyr)
library(lubridate)

sf_tracts_with_weather_time <- sf_tracts_with_weather %>%
  mutate(
    # parse the ISO string as UTC
    datetime_utc = ymd_hms(date, tz = "UTC"),
    # shift that instant into Pacific Time (handles PST vs PDT)
    datetime_pst = with_tz(datetime_utc, tzone = "America/Los_Angeles"),
    # now pull out each component in local time
    date_pst = as.Date(datetime_pst),
    Year   = year(datetime_pst),
  ) %>%
  # optional: drop the helper columns if you don’t need them
  select(-datetime_utc)

```


## We can also plot the weather data using ggplot2. 

```{r, plot.weather, echo = FALSE}
library(ggplot2)
library(dplyr)

target_date <- "1990-08-01" # selecting a specific date 

# If you want to map all datatypes (e.g., TMAX & TMIN), you can facet by datatype
tracts_day <- sf_tracts_with_weather_time %>%
  filter(date_pst == target_date)

ggplot(tracts_day) +
  geom_sf(aes(fill = interpolated_value_f), color = "white", size = 0.1) +
  scale_fill_viridis_c(
    na.value = "grey80",
    name = "Weather value"
  ) +
  labs(
    title = paste("Interpolated Weather by Tract on", target_date),
    fill = "Value"
  ) +
  facet_wrap(~datatype) +         # Remove this line if you only have one datatype!
  theme_minimal()

```


## We can also download the package `rnoaa` instead of using the API. 
 
```{r}

install.packages("~/Library/Mobile Documents/com~apple~CloudDocs/Documents/SFSU Faculty/PINC 2025/PSP2025/rnoaa_1.4.0.tar.gz", type = "source", repos = NULL)

pacman::p_load(crul, XML, isdparser, geonames, hoardr)

library(rnoaa)

```



```{r}
options(noaakey = "sYSaRlnFshYqsTcobQCAivNIaHqEUWVc")

ncdc_locs(locationcategoryid='CITY', sortfield='name', sortorder='desc')


ncdc_stations(datasetid='GHCND', locationid='FIPS:06075')

sf_stations <- ncdc_stations(datasetid='GHCND', locationid='FIPS:06075')

sf_stations_df <- sf_stations$data$id

# SF county FIPS 06075 

# City ID CITY:US060031


ncdc_stations(datasetid='GHCND', locationid='FIPS:06075') 


lapply(c("GHCND:US1CASF0003", "GHCND:US1CASF0004", "GHCND:US1CASF0008", "GHCND:US1CASF0014", "GHCND:US1CASF0017", "GHCND:US1CASF0020", "GHCND:US1CASF0021", "GHCND:USC00047765", "GHCND:USC00047767", "GHCND:USW00023272"), function(z) {
  ncdc_stations(
   startdate = "2013-01-01",
   enddate = "2014-11-01")
}$data)


# to do: plot the 10 locations and verify that they are in SF 

# 1 %in% unlist(A)

sf_stations_df %in% df_data_stations

df_data_stations %in% sf_stations_df


out <- ncdc(
  datasetid = 'NORMAL_DLY',
  stationid = 'GHCND:USW00023272', # downtown SF 
  datatypeid = 'dly-tmax-normal',
  startdate = '2000-01-01',
  enddate = '2000-02-01'
)


ncdc_datacats(locationid = 'CITY:US060031')


sf_data_2000 <- ncdc(datasetid = "NORMAL_DLY", stationid = sf_stations$data$id[1:10],
        datatypeid='dly-tmax-normal',
   startdate = "2000-01-01", enddate = "2000-12-31")


```



## Daymet R package

The Daymet package can be used to download gridded weather data. Unlike NOAA (National Oceanic and Atmospheric Administration), which uses observational climate and weather data collected from ground stations, satellites, and other sources, often at daily or hourly resolution. Daymet provides gridded daily surface weather data (e.g., temperature, precipitation) for North America, derived from interpolation of ground-based station data, and is especially useful for ecological and environmental modeling where spatially continuous data are needed.

More information and examples of how to use the R package can be found here: https://cran.r-project.org/web/packages/daymetr/vignettes/daymetr-vignette.html

To download the data we need to select the locations of the estimates. Given we want to examine weather at the census tract level, we can use census tract centroids to extract estimates at these locations. 

First, we created a CSV file with information needed for batch downloading using the Daymet R package. 

```{r}
# make csv file with : site (GEOID), latitude, longitude

sf_centroids <- tract_centroids %>% select(GEOID, INTPTLAT, INTPTLON)

sf_centroid_df <- st_drop_geometry(sf_centroids)

sf_centroid_df <- sf_centroid_df %>% mutate(across(c(INTPTLAT, INTPTLON), as.numeric))

sf_centroid_df <- sf_centroid_df %>% 
                        rename(site = GEOID,
                               lat = INTPTLAT,
                               lon = INTPTLON)


write.csv(sf_centroid_df, "sf_centroids.csv", row.names = FALSE)


```


```{r}

# if(!require(remotes)){install.packages("remotes")}
# remotes::install_github("bluegreen-labs/daymetr")
# 

library("daymetr")


df_batch <- download_daymet_batch(file_location = "~/Library/Mobile Documents/com~apple~CloudDocs/Documents/SFSU Faculty/PINC 2025/PSP2025/Climate Change/sf_centroids.csv", 
start = 2000, 
end = 2024, 
simplify = TRUE)


write.csv(df_batch, "sf_daymet.csv", row.names = FALSE)


```



# Combining NOAA and Daymet data 

| Step | Action                                                                              |
| ---- | ----------------------------------------------------------------------------------- |
| 1    | Plot and compare tmax from NOAA vs. Daymet at overlapping points                    |
| 2    | Use Daymet for full coverage of tract-level exposure from 2010–2020                 |
| 3    | Use NOAA for validation at specific weather stations                                |
| 4    | Mention in methods that you compared the two, and why you chose Daymet for modeling |
| 5    | Optionally run sensitivity analyses using NOAA in a subset of tracts                |



1. Compare the Two Sources
Start by comparing them to understand how they differ in:


| Feature           | NOAA                   | Daymet                        |
| ----------------- | ---------------------- | ----------------------------- |
| Source            | Ground stations        | Gridded interpolation (1 km²) |
| Spatial coverage  | Sparse, station-based  | Complete for all tracts       |
| Temporal coverage | Varies by station      | Consistent daily data         |
| Variables         | tmax, tmin, prcp, etc. | Same, but interpolated        |

Plot daily tmax from NOAA vs. Daymet at the same location(s) over a sample year (e.g., 2015) to check how closely they align.

2. Decide Which to Use for Main Analysis
Ask:

Does NOAA have gaps or missing tracts?

Do you need tract-level coverage (not just near stations)?

Use Daymet for full-coverage, tract-level exposure analysis — especially since your data is matched to census tract centroids.

Use NOAA as a validation/check or for sensitivity analysis in key locations.


3. Use NOAA for Sensitivity or Ground-Truthing
Using the two NOAA stations within San Francisco

Compare results (e.g., number of extreme heat days, trends) with the same time/area in Daymet

Report discrepancies or alignment — this increases confidence in your exposure estimates



## Validation of data

We compare Daymet and NOAA data to validate the accuracy of the Daymet estimates used in our analysis. Since Daymet data is interpolated from ground-based stations—including NOAA, it is important to confirm that its temperature values align closely with actual observations from NOAA stations in San Francisco. This validation step builds confidence in using Daymet as the primary source for modeling heat exposure across all census tracts, especially where NOAA data may not be available. It also helps us quantify any systematic differences or biases, ensuring transparency and rigor in our methodological approach.

What to Look For: Correlation > 0.9 is excellent

Low RMSE / MAE shows good agreement

Bias ≈ 0 means Daymet isn’t systematically too high/low
 
```{r}
# Ensure dates are in Date format
daymet_df <- daymet_df %>%
  mutate(date = as.Date(date))

noaa_df <- noaa_df %>%
  mutate(date = as.Date(date))

# Join both datasets by site and date
compare_df <- inner_join(daymet_df, noaa_df, by = c("site", "date"))

# Plot tmax values from both sources
ggplot(compare_df, aes(x = date)) +
  geom_line(aes(y = tmax_daymet, color = "Daymet")) +
  geom_line(aes(y = tmax_noaa, color = "NOAA")) +
  facet_wrap(~ site, scales = "free_y") +
  labs(title = "Daymet vs NOAA Daily Tmax",
       y = "Tmax (°C)", x = "Date", color = "Data Source") +
  theme_minimal()


# Correlation and RMSE per site
library(Metrics)

validation_stats <- compare_df %>%
  group_by(site) %>%
  summarise(
    correlation = cor(tmax_daymet, tmax_noaa, use = "complete.obs"),
    rmse = rmse(tmax_noaa, tmax_daymet),
    mae = mae(tmax_noaa, tmax_daymet),
    bias = mean(tmax_daymet - tmax_noaa, na.rm = TRUE)
  )

print(validation_stats)


```
 
 
 
 

```{r session.info, echo=FALSE}
#-----session info-----

sessionInfo()
```


