---
title: "Climate Change Data"
author: "Nancy Carmona"
date: "`r format(Sys.time(), '%B %d, %Y')`"
output:
  pdf_document: default
  html_document: default
---

```{r set.knitr.options, echo=FALSE, message=FALSE, warning=FALSE}
#-----set knitr options-----

# clear environment
rm(list = ls())

# knitr options
suppressMessages(library(knitr))
opts_chunk$set(tidy = FALSE, echo = TRUE, message = FALSE)

# Load pacman, installing if needed and other packages as needed
if (!require("pacman")) { install.packages("pacman", repos = "http://cran.r-project.org") }

# Load other packages, installing as needed.
pacman::p_load(tidyverse, dplyr, httr, jsonlite, tigris, sf, gstat)

# # set working directory
# work_dir <- getwd()
# 
# # name and create output directory
# output_dir <- file.path(work_dir, "output")
# dir.create(output_dir, showWarnings = TRUE, recursive = TRUE)

```


```{r token, echo = FALSE}
# Set your NOAA API Token (replace with your own API key)
noaa_token <- "sYSaRlnFshYqsTcobQCAivNIaHqEUWVc"

# Define the API endpoint URL (get this from NOAA website https://www.ncdc.noaa.gov/cdo-web/webservices/v2)

# base_url <- "https://www.ncdc.noaa.gov/cdo-web/api/v2/data"

# url from NOAA website, either can be used https://www.ncdc.noaa.gov/cdo-web/webservices/v2


```


## Downloading data from an API 

```{r define API, echo = FALSE}

noaa_token <- "sYSaRlnFshYqsTcobQCAivNIaHqEUWVc"


fetch_noaa_data_paginated <- function(start_date, end_date, location_id, datatypeids, token) {
  base_url <- "https://www.ncdc.noaa.gov/cdo-web/api/v2/data"
  all_results <- list()
  offset <- 1
  limit <- 1000
  more_data <- TRUE
  
  # Create the base query
  params <- list(
    datasetid = "GHCND",
    locationid = location_id,
    startdate = start_date,
    enddate = end_date,
    units = "metric",
    limit = limit,
    offset = offset
  )
  
  # Build the query string with multiple datatypeids
  datatypeid_params <- paste(paste0("datatypeid=", datatypeids), collapse = "&")
  
  while(more_data) {
    # Build the full URL with parameters
    full_url <- modify_url(
      base_url,
      query = c(params, list(offset = offset))
    )
    # Add the datatypeids manually
    full_url <- paste0(full_url, "&", datatypeid_params)
    
    response <- GET(full_url, add_headers(token = token))
    if (status_code(response) != 200) {
      cat("Error: Status", status_code(response), "\n")
      break
    }
    results <- fromJSON(content(response, "text", encoding = "UTF-8"))
    if (!is.null(results$results)) {
      all_results <- append(all_results, list(results$results))
      num_results <- length(results$results)
      cat("Fetched", num_results, "records (offset", offset, ")\n")
      offset <- offset + limit
      if (num_results < limit) {
        more_data <- FALSE
      }
    } else {
      more_data <- FALSE
    }
    Sys.sleep(1)
  }
  if (length(all_results) > 0) {
    return(bind_rows(all_results))
  } else {
    return(NULL)
  }
}

# Example usage (loop by year, API works better if 1 year or less of data being requested at a time)
all_data <- list()
for (yr in 1990:2024) {
  cat("Downloading data for year:", yr, "\n")
  res <- fetch_noaa_data_paginated(
    start_date = paste0(yr, "-01-01"),
    end_date = paste0(yr, "-12-31"),
    location_id = "CITY:US060031",
    datatypeids = c("TMAX", "TMIN"),
    token = noaa_token
  )
  if (!is.null(res)) {
    all_data[[as.character(yr)]] <- res
  }
}
sf_weather <- bind_rows(all_data)

# save the data as a CSV to use for later 
write.csv(sf_weather, "sf_weather_noaa_1990_2024.csv", row.names = FALSE)

```


## Join with stations 

```{r combine.data, echo = FALSE}

response <- GET(
  url = "https://www.ncei.noaa.gov/cdo-web/api/v2/stations",
  query = list(
    locationid = "CITY:US060031",   # San Francisco city code
    datasetid = "GHCND",
    limit = 1000
  ),
  add_headers(token = noaa_token)
)

stations_json <- fromJSON(content(response, "text", encoding = "UTF-8"))
sf_stations_api <- stations_json$results

# Show the resulting stations
sf_station_ids <- sf_stations_api$id
sf_weather_only <- sf_weather %>% filter(station %in% sf_station_ids)
# Convert sf_stations_api to a tibble/data frame if not already
sf_stations_df <- as_tibble(sf_stations_api)

# Join weather data to station metadata (by station ID)
sf_weather_with_meta <- sf_weather_only %>%
  left_join(sf_stations_df, by = c("station" = "id"))


# Convert to Spatial Data
weather_points <- st_as_sf(sf_weather_with_meta, coords = c("longitude", "latitude"), crs = 4326)



# Get 2020 census tracts for San Francisco County (FIPS: 075)
sf_tracts <- tracts(state = "CA", county = "San Francisco", year = 2020, class = "sf")

plot(sf_tracts$geometry)

sf_tracts <- st_transform(sf_tracts, crs = 4326)

```


```{r interpolate, echo = FALSE}
library(sf)
library(gstat)
library(dplyr)
library(purrr)

tract_centroids <- st_centroid(sf_tracts)

interpolate_one <- function(datatype, date) {
  day_pts <- weather_points %>%
    filter(datatype == !!datatype, date == !!date, !is.na(value))
  if (nrow(day_pts) < 2) return(NULL) # skip if not enough points
  
  # Make sure CRS matches
  day_pts <- st_transform(day_pts, 4326)
  centroids <- st_transform(tract_centroids, 4326)
  
  idw_result <- gstat::idw(
    value ~ 1,
    day_pts,
    newdata = centroids,
    idp = 2.0
  )
  tibble(
    GEOID = centroids$GEOID,
    date = date,
    datatype = datatype,
    interpolated_value = idw_result$var1.pred
  )
}

# Loop as before using purrr
datatypes <- unique(weather_points$datatype)
all_dates <- unique(weather_points$date)
grid <- expand.grid(datatype = datatypes, date = all_dates, stringsAsFactors = FALSE)
interpolated_results <- pmap_dfr(
  grid,
  function(datatype, date) interpolate_one(datatype, date)
)

```


```{r combine.tracts, echo= FALSE}

sf_tracts_with_weather <- left_join(sf_tracts, interpolated_results, by = "GEOID")

```


## Transform units

```{r, transform.f, echo = FALSE}
library(dplyr)

sf_tracts_with_weather <- sf_tracts_with_weather %>%
  mutate(
    interpolated_value_f = (interpolated_value / 10) * 9/5 + 32,
  )

```

## Fixing Time 
```{r, transform.time, echo = FALSE}
library(dplyr)
library(lubridate)

sf_tracts_with_weather_time <- sf_tracts_with_weather %>%
  mutate(
    # parse the ISO string as UTC
    datetime_utc = ymd_hms(date, tz = "UTC"),
    # shift that instant into Pacific Time (handles PST vs PDT)
    datetime_pst = with_tz(datetime_utc, tzone = "America/Los_Angeles"),
    # now pull out each component in local time
    date_pst = as.Date(datetime_pst),
    Year   = year(datetime_pst),
  ) %>%
  # optional: drop the helper columns if you donâ€™t need them
  select(-datetime_utc)

```



```{r, plot.weather, echo = FALSE}
library(ggplot2)
library(dplyr)

target_date <- "1990-08-01"

# If you want to map all datatypes (e.g., TMAX & TMIN), you can facet by datatype
tracts_day <- sf_tracts_with_weather_time %>%
  filter(date_pst == target_date)

ggplot(tracts_day) +
  geom_sf(aes(fill = interpolated_value_f), color = "white", size = 0.1) +
  scale_fill_viridis_c(
    na.value = "grey80",
    name = "Weather value"
  ) +
  labs(
    title = paste("Interpolated Weather by Tract on", target_date),
    fill = "Value"
  ) +
  facet_wrap(~datatype) +         # Remove this line if you only have one datatype!
  theme_minimal()

```



```{r session.info, echo=FALSE}
#-----session info-----

sessionInfo()
```


